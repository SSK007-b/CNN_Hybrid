{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71ee1501-7860-4aa5-82b1-39e344e740fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e661664-7427-4817-b4b6-e7e40b46abe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "def create_cnn_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', padding = \"same\", input_shape=input_shape))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu', padding = \"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((3, 3) , strides=2))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding = \"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((3, 3) , strides=2))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu', padding = \"same\"))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3e7022a-9747-47eb-945e-0a8e137c463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Transformer model\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "             layers.Dense(ff_dim, activation=\"relu\"), \n",
    "             layers.Dense(embed_dim)\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization()\n",
    "        self.layernorm2 = layers.LayerNormalization()\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf069f9-8806-462b-926c-ed286dc0beb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_transformer_blocks):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = (inputs)\n",
    "    # x = layers.LayerNormalization()(x)\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = TransformerEncoder(embed_dim, num_heads, ff_dim)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation = \"relu\")(x)\n",
    "    outputs = layers.Dense(2, activation='softmax')(x)\n",
    "    return models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edd2994-01ed-4e3b-9dc1-cdd0201fa07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CNN and Transformer\n",
    "def create_cnn_transformer_model(input_shape, embed_dim, num_heads, ff_dim, num_transformer_blocks):\n",
    "    cnn_model = create_cnn_model(input_shape)\n",
    "    cnn_output_shape = cnn_model.layers[-1].output_shape[1:]\n",
    "    transformer_model = create_transformer_model(cnn_output_shape, embed_dim, num_heads, ff_dim, num_transformer_blocks)\n",
    "    \n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = cnn_model(inputs)\n",
    "    outputs = transformer_model(x)\n",
    "\n",
    "    return models.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2215c011-93ea-4e69-858b-8d59d99c00b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 221357 images belonging to 2 classes.\n",
      "Found 55338 images belonging to 2 classes.\n",
      "(50, 50, 3)\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=False,\n",
    "                            validation_split = 0.2)\n",
    "train_iterator = datagen.flow_from_directory(\"D:\\Transformers\\Datasets\",target_size=(50, 50),batch_size = 32, subset = \"training\")\n",
    "x_train, y_train = train_iterator.next()\n",
    "test_iterator = datagen.flow_from_directory(\"D:\\Transformers\\Datasets\",target_size=(50, 50),batch_size = 32, subset = \"validation\")\n",
    "x_test, y_test = test_iterator.next()\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/225.0\n",
    "input_shape = x_train.shape[1:]\n",
    "print(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1163b880-09b3-420f-b95b-9a9b0157499d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "model = create_cnn_transformer_model(input_shape, embed_dim=128, num_heads=2, ff_dim=(128*2), num_transformer_blocks=5)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa23a9c3-2fb9-454a-995a-865d251067bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 32s 32s/step - loss: 0.7779 - accuracy: 0.6562\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 445ms/step - loss: 2.2808 - accuracy: 0.5938\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 458ms/step - loss: 0.6271 - accuracy: 0.7812\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 0.9668 - accuracy: 0.5625\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 490ms/step - loss: 0.1790 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 1s 508ms/step - loss: 0.1873 - accuracy: 0.9062\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 485ms/step - loss: 0.1113 - accuracy: 0.9688\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 0.0633 - accuracy: 0.9688\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 0.0247 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 479ms/step - loss: 0.0435 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23a1179e510>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size = 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ada36b9c-aaa4-49fe-ba06-f9062ab23a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step - loss: 0.9662 - accuracy: 0.8125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9662061929702759, 0.8125]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4ddc668-3498-4e82-961b-716b68eee8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33dce9cf-821a-41f7-b13e-5f0c4f997e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.81      0.81        32\n",
      "         1.0       0.81      0.81      0.81        32\n",
      "\n",
      "    accuracy                           0.81        64\n",
      "   macro avg       0.81      0.81      0.81        64\n",
      "weighted avg       0.81      0.81      0.81        64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_test = np.reshape(y_test, newshape = 64)\n",
    "y_pred = np.reshape(y_pred, newshape = 64)\n",
    "y_pred = np.where(y_pred >= 0.5, 1, 0)\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
